{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling the Toronto OpenStreetMap dataset with MongoDB\n",
    "\n",
    "Anton Rubisov\n",
    "\n",
    "URL to Toronto dataset: https://mapzen.com/data/metro-extracts/metro/toronto_canada/  \n",
    "URL to Financial District dataset: https://www.openstreetmap.org/relation/5446604\n",
    "\n",
    "The aim of this project is to audit (for validity, accuracy, completeness, consistency, and uniformity), clean, and explore the Toronto OpenStreetMap dataset. The full XML dump for this region is 1.2GB, and so to faciliate the auditing, I began by investigating a reduced area - the Toronto Financial District, where I work - that is only 15MB. Even here, I initially explored only 1/10th of the file in order to get a sense of where cleaning might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Cleaning\n",
    "\n",
    "Evaluating Data Quality:  \n",
    "Validity - data conforms to a schema (or other constraints)  \n",
    "Accuracy - data conforms to gold standard  \n",
    "Completeness - do we have all records we should have  \n",
    "Consistency - consistency among fields that overlap  \n",
    "Uniformity - same units used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from create_reduced_dataset import *\n",
    "from audit_data_quality import *\n",
    "\n",
    "fin_dist = \"financial_district.osm\"\n",
    "create_reduced_dataset(fin_dist, out_file=\"fin_dist_reduced.osm\", k=10)\n",
    "data_reduced = audit_key_types('fin_dist_reduced.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I'll do is inspect the `node` and `way` tag keys. This'll give me a sense of what values I'm looking at, and where effort needs to be applied. It turns out there are 247 unique tag keys in just this reduced dataset, some of which have tens of entries, and a full reproduction of the `data_reduced` variable here is not appropriate - however, even a high level view of the address counts is informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addr:city': 253,\n",
      " 'addr:country': 191,\n",
      " 'addr:floor': 1,\n",
      " 'addr:housename': 5,\n",
      " 'addr:housenumber': 400,\n",
      " 'addr:inclusion': 1,\n",
      " 'addr:interpolation': 98,\n",
      " 'addr:postcode': 31,\n",
      " 'addr:province': 156,\n",
      " 'addr:state': 40,\n",
      " 'addr:street': 406,\n",
      " 'addr:unit': 2}\n"
     ]
    }
   ],
   "source": [
    "display_address_counts(data_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary is simply counting the number of times a particular tag appears in any of the nodes. The first observation is that the number of house numbers (400) is less than the number of street names (406), indicating that 6 nodes don't even have street numbers attached to them. Extending on this, full addresses are in short supply, as the number of city, country, and postal code entries dwindles off from there. Thus, addressing the question of data completeness, many addresses are clearly incomplete. \n",
    "\n",
    "Even here we can see that data uniformity is lacking: some addresses have a 'state' value while many more have a 'province' value. Being as we are in Canada, all addresses should have a province rather than a state.\n",
    "\n",
    "Exploring the address data a little further for validity, both the city and province attributes need to be standardized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 253, 'vals': {'Toronto', 'City of Toronto'}}\n",
      "{'count': 156, 'vals': {'Ontario', 'ON'}}\n"
     ]
    }
   ],
   "source": [
    "print(data_reduced['addr:city'])\n",
    "print(data_reduced['addr:province'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, postal codes, where available, are in good order and format. \n",
    "\n",
    "Another small address issue, also with data validity, is found in street names. The street that runs along the harbor front is called Queens Quay, and appears in the dictionary of street names under both that name and Queen's Quay. This is hardly surprising, as Torontonians seldom appreciate the distinction, and it even appears under both guises in Wikipedia: [Queens Quay](https://en.wikipedia.org/wiki/Queens_Quay_(Toronto&#41;) and [Queen's Quay Terminal](https://en.wikipedia.org/wiki/Queen%27s_Quay_Terminal). This'll likely be an issue with the even more misunderstood [Princes' Gates](https://en.wikipedia.org/wiki/Exhibition_Place#Princes.27_Gates).\n",
    "\n",
    "Two other areas that I will focus on for cleanup are `phone` and `opening_hours`. Here the data does not conform to a single schema for format, and this will need to be standardized in the cleanup phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 25,\n",
      " 'vals': {'(416) 367-0685',\n",
      "          '(647) 896-1774',\n",
      "          '+1 (416) 362-1739',\n",
      "          '+1 (416) 365-0481',\n",
      "          '+1 (647) 352 8802',\n",
      "          '+1 416 642 8321',\n",
      "          '+1 416 861-1211',\n",
      "          '+1 416 864 7791',\n",
      "          '+1 416 916 8811',\n",
      "          '+1 416-977-6000 Ext. 2208',\n",
      "          '+1-416-597-0366',\n",
      "          '+1-416-865-9700',\n",
      "          '+1-416-913-8880',\n",
      "          '+1-416-941-8920',\n",
      "          '416 862 7575',\n",
      "          '416-306-0335',\n",
      "          '416-348-8887',\n",
      "          '416-367-0645',\n",
      "          '416-546-2200',\n",
      "          '416-591-0404',\n",
      "          '416-593-2560',\n",
      "          '416-599-3334',\n",
      "          '416-862-8899',\n",
      "          '416.977.3222',\n",
      "          '4163638330'}}\n",
      "{'count': 15,\n",
      " 'vals': {'24/7',\n",
      "          'Minday-Friday : 08:00-19:00',\n",
      "          'Mo-Fr 06:30-18:00; Sa-Su 10:00-16:00',\n",
      "          'Mo-Fr 07:00-23:00',\n",
      "          'Mo-Fr 08:00-18:00; Tu,Th 08:00-19:00; Sa 09:00-13:00',\n",
      "          'Mo-Fr 6:30-19:00; Sa-Su 8:00-18:00',\n",
      "          'Mo-Fr 7:45-17:00; Sa-Su off',\n",
      "          'Mo-Fr 8:00-17:00',\n",
      "          'Mo-Fr 9:00-19:00; Sa-Su off',\n",
      "          'Mo-Sa 10:00-18:00; Su off',\n",
      "          'Mo-Su 11:00-22:00',\n",
      "          'Mo-Th 00:00-01:00,11:00-24:00; Fr-Su 00:00-02:00,11:00-24:00',\n",
      "          'We-Mo 07:00-20:00, Tu 07:00-19:00'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(data_reduced['phone'])\n",
    "pprint.pprint(data_reduced['opening_hours'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the phone number format, we look to the Canadian national convention for phone numbers: http://www.bt-tb.tpsgc-pwgsc.gc.ca/btb.php?lang=eng&cont=044 As the OpenStreetMap project extends to an international audience, we'll follow the guideline for international formatting: \"The Translation Bureau recommends writing a Canadian telephone number in the following international format, without hyphens: + 1 819 555 5555\" Thus our phones get reformatted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             416-599-3334  ->  +1 416 599 3334\n",
      "             416-591-0404  ->  +1 416 591 0404\n",
      "             416-593-2560  ->  +1 416 593 2560\n",
      "           (647) 896-1774  ->  +1 647 896 1774\n",
      "             416 862 7575  ->  +1 416 862 7575\n",
      "          +1 416 916 8811  ->  +1 416 916 8811\n",
      "        +1 (647) 352 8802  ->  +1 647 352 8802\n",
      "        +1 (416) 362-1739  ->  +1 416 362 1739\n",
      "             416-306-0335  ->  +1 416 306 0335\n",
      "          +1-416-597-0366  ->  +1 416 597 0366\n",
      "+1 416-977-6000 Ext. 2208  ->  +1 416 977 6000 Ext. 2208\n",
      "          +1-416-913-8880  ->  +1 416 913 8880\n",
      "          +1-416-941-8920  ->  +1 416 941 8920\n",
      "          +1 416 642 8321  ->  +1 416 642 8321\n",
      "             416-546-2200  ->  +1 416 546 2200\n",
      "             416.977.3222  ->  +1 416 977 3222\n",
      "             416-862-8899  ->  +1 416 862 8899\n",
      "             416-367-0645  ->  +1 416 367 0645\n",
      "        +1 (416) 365-0481  ->  +1 416 365 0481\n",
      "             416-348-8887  ->  +1 416 348 8887\n",
      "           (416) 367-0685  ->  +1 416 367 0685\n",
      "               4163638330  ->  +1 416 363 8330\n",
      "          +1 416 861-1211  ->  +1 416 861 1211\n",
      "          +1-416-865-9700  ->  +1 416 865 9700\n",
      "          +1 416 864 7791  ->  +1 416 864 7791\n"
     ]
    }
   ],
   "source": [
    "from format_phones import *\n",
    "\n",
    "for phone in data_reduced['phone']['vals']:\n",
    "    print(\"{:>25}  ->  {}\".format(phone,parse_phone_number(phone)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to opening hours, this format is a lot more difficult to massage into a single schema. According to http://wiki.openstreetmap.org/wiki/Key:opening_hours, we want days of the week identified by a two-character abbreviation; day ranges identified by a hyphen and multiple dates separated by a comma (e.g. Mo-Fr and Tu,Th); times by the 24h clock with a leading zero if before noon; multiple time ranges per entry separated by a comma (e.g. 00:00-01:00,11:00-24:00); multiple entries separated by a semi-colon (e.g. Mo-Fr 06:30-18:00; Sa-Su 10:00-16:00); closures identified by absence of entries rather than an 'off' entry (e.g. Sa-Su off); and no overlap in the dates in the entries (e.g. Mo-Fr 08:00-18:00; Tu,Th 08:00-19:00 overlaps Tu and Th because they're also in the Mo-Fr range). And those are just the simple values - the documentation gets vastly more complex in treating odd cases, which for this project we'll just leave unchanged. Thus, our data becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mo-Fr 8:00-17:00\n",
      "->  Mo-Fr 08:00-17:00\n",
      "\n",
      "    We-Mo 07:00-20:00, Tu 07:00-19:00\n",
      "->  Mo,We-Su 07:00-20:00; Tu 07:00-19:00\n",
      "\n",
      "    Minday-Friday : 08:00-19:00\n",
      "->  Mo-Fr 08:00-19:00\n",
      "\n",
      "    Mo-Sa 10:00-18:00; Su off\n",
      "->  Mo-Sa 10:00-18:00\n",
      "\n",
      "    Mo-Fr 06:30-18:00; Sa-Su 10:00-16:00\n",
      "->  Mo-Fr 06:30-18:00; Sa-Su 10:00-16:00\n",
      "\n",
      "    Mo-Fr 07:00-23:00\n",
      "->  Mo-Fr 07:00-23:00\n",
      "\n",
      "    Mo-Su 11:00-22:00\n",
      "->  Mo-Su 11:00-22:00\n",
      "\n",
      "    Mo-Fr 7:45-17:00; Sa-Su off\n",
      "->  Mo-Fr 07:45-17:00\n",
      "\n",
      "    Mo-Fr 08:00-18:00; Tu,Th 08:00-19:00; Sa 09:00-13:00\n",
      "->  Mo,We,Fr 08:00-18:00; Tu,Th 08:00-19:00; Sa 09:00-13:00\n",
      "\n",
      "    Mo-Th 00:00-01:00,11:00-24:00; Fr-Su 00:00-02:00,11:00-24:00\n",
      "->  Mo-Th 00:00-01:00,11:00-24:00; Fr-Su 00:00-02:00,11:00-24:00\n",
      "\n",
      "    Mo-Fr 9:00-19:00; Sa-Su off\n",
      "->  Mo-Fr 09:00-19:00\n",
      "\n",
      "    Mo-Fr 6:30-19:00; Sa-Su 8:00-18:00\n",
      "->  Mo-Fr 06:30-19:00; Sa-Su 08:00-18:00\n",
      "\n",
      "    24/7\n",
      "->  Mo-Su 00:00-24:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from format_hours import *\n",
    "\n",
    "for hours in data_reduced['opening_hours']['vals']:\n",
    "    print(\"    {}\\n->  {}\\n\".format(hours,parse_opening_hours(hours)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Regex, the code in fact does a rather good job of getting the opening hours into the format that we want, including capturing hours on public holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the strings have critical information missing (e.g. open Mo-Su but at what hours? Open 11:00-23:00 but on what days?). The last few strings require parsing the 12-hour clock times into 24-hour times, as well as dealing with the additional hyphen between the day names and times, and I leave this to a future iteration of this project.\n",
    "\n",
    "In summary, our data cleanup at this stage will require the following:\n",
    "1. Parsing and expanding abbreviations in street names (incl. mapping Queen's Quay to Queens Quay)\n",
    "2. Filling in missing city, province, and country values\n",
    "3. Updating existing city, province, and country values to conform to a single schema\n",
    "4. Updating existing phone numbers to conform to a single schema\n",
    "5. Updating business opening hours to conform to a single schema\n",
    "\n",
    "Once cleaned, the Toronto data was converted into a JSON format and pushed into a MongoDB database using the command:  \n",
    "\n",
    "<code>mongoimport --db udacity --collection toronto --file toronto_canada.osm.json</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A note on using ElementTree.IterParse: due to the size of the OSM file, the cleaning script perpetually froze my computer when attempting to work with the full dataset - after a couple reboots, I determined this was from the script utilizing all my RAM. The solution was:  \n",
    "<pre>\n",
    "for _, element in ET.iterparse(file_in, events=(\"start\",)):\n",
    "    # if element.tag == 'node':\n",
    "        ...\n",
    "        \n",
    "    element.clear() \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearing the element from memory eliminated RAM utilization entirely.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Overview of Data in MongoDB\n",
    "\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them.\n",
    "\n",
    "<pre>File sizes\n",
    "                                                \n",
    "toronto_canada.osm ......... 1182 MB\n",
    "toronto_canada.osm.json .... 1299 MB</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents some of the basic statistics on the dataset obtained by running the mongo shell in bash:\n",
    "\n",
    "#### Number of records\n",
    "<code> \\> db.toronto.find().count()\n",
    "5647295\n",
    "</code>\n",
    "\n",
    "#### Number of nodes\n",
    "<code> \\> db.toronto.find({\"type\":\"node\"}).count()\n",
    "4922735\n",
    "</code>\n",
    "\n",
    "#### Number of ways\n",
    "<code> \\> db.toronto.find({\"type\":\"way\"}).count()\n",
    "720747\n",
    "</code>\n",
    "                                                \n",
    "#### Number of unique contributors\n",
    "<code> \\> db.toronto.distinct(\"created.user\").length\n",
    "2179\n",
    "</code>\n",
    "\n",
    "#### Being as we are in Canada -- number of Tim Hortons\n",
    "<code> \\> db.toronto.find({\"name\":\"Tim Hortons\"}).count()\n",
    "611\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore some of the data a little further, and for the sake of documentation, access some of the figures using queries through PyMongo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': None, 'count': 286160},\n",
      " {'_id': 'Toronto', 'count': 119847},\n",
      " {'_id': 'Hamilton', 'count': 40332},\n",
      " {'_id': 'Mississauga', 'count': 36935},\n",
      " {'_id': 'Brampton', 'count': 28856},\n",
      " {'_id': 'Vaughan', 'count': 17332},\n",
      " {'_id': 'Town of Markham', 'count': 16732},\n",
      " {'_id': 'Burlington', 'count': 15944},\n",
      " {'_id': 'Oakville', 'count': 15637},\n",
      " {'_id': 'Oshawa', 'count': 13930}]\n"
     ]
    }
   ],
   "source": [
    "from mongo_interface import *\n",
    "db = get_db('udacity')\n",
    "\n",
    "# Of the contents of this dataset, which extends beyond the municipal boundaries\n",
    "# of the City of Toronto, what proportion fall into which township?\n",
    "pipeline = [{'$match': {'address': {'$exists': 1}}},\n",
    "            {'$group': {'_id': '$address.city',\n",
    "                        'count': {'$sum': 1} } },\n",
    "            {'$sort': { 'count': -1 } },\n",
    "            {'$limit': 10}]\n",
    "pprint(aggregate(db, pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': \"McDonald's\", 'count': 9},\n",
      " {'_id': 'Tim Hortons', 'count': 8},\n",
      " {'_id': 'The Bagel House', 'count': 2},\n",
      " {'_id': 'A&W', 'count': 2},\n",
      " {'_id': '7 West', 'count': 1},\n",
      " {'_id': \"Fran's Restaurant\", 'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Limiting ourselves to just Toronto proper, what restaurants are open 24/7?\n",
    "pipeline = [{'$match': {'address.city': 'Toronto',\n",
    "                        'opening_hours': 'Mo-Su 00:00-24:00',\n",
    "                        'amenity': {'$in': ['fast_food','cafe','restaurant'] }}},\n",
    "            {'$group': {'_id': '$name',\n",
    "                        'count': {'$sum': 1} } },\n",
    "            {'$sort': { 'count': -1 } }]\n",
    "pprint(aggregate(db, pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'Toronto Transit Commission', 'count': 5343},\n",
      " {'_id': 'York Region Transit', 'count': 1562},\n",
      " {'_id': 'MiWay', 'count': 1531},\n",
      " {'_id': 'Durham Region Transit', 'count': 688},\n",
      " {'_id': None, 'count': 450},\n",
      " {'_id': 'Hamilton Street Railway', 'count': 355},\n",
      " {'_id': 'Brampton Transit', 'count': 323},\n",
      " {'_id': 'Oakville Transit', 'count': 259},\n",
      " {'_id': 'GO Transit', 'count': 229},\n",
      " {'_id': 'Burlington Transit', 'count': 134},\n",
      " {'_id': 'Milton Transit', 'count': 126},\n",
      " {'_id': 'VIVA', 'count': 73},\n",
      " {'_id': 'York Region Transit, GO Transit', 'count': 64},\n",
      " {'_id': 'York Region Transit (contracted)', 'count': 48},\n",
      " {'_id': 'Durham Region Transit; GO Transit', 'count': 40},\n",
      " {'_id': 'Toronto Transit Commission, York Region Transit', 'count': 40},\n",
      " {'_id': 'Zum (Brampton Transit)', 'count': 27},\n",
      " {'_id': 'York Region Transit, VIVA', 'count': 23},\n",
      " {'_id': 'Toronto Transit Commision', 'count': 22},\n",
      " {'_id': 'Durham Region Transit, GO Transit', 'count': 19}]\n"
     ]
    }
   ],
   "source": [
    "# What is the size of the transport networks in the dataset?\n",
    "pipeline = [{'$match': {'$or': [ {'highway': 'bus_stop'}, {'railway': 'tram_stop'}] } },\n",
    "            {'$group': {'_id': '$operator',\n",
    "                        'count': {'$sum': 1} } },\n",
    "            {'$sort': {'count': -1}},\n",
    "            {'$limit': 20}]\n",
    "pprint(aggregate(db, pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'Yonge Street', 'count': 2267},\n",
      " {'_id': 'Bathurst Street', 'count': 1089},\n",
      " {'_id': 'Dundas Street West', 'count': 1035}]\n"
     ]
    }
   ],
   "source": [
    "# Yonge Street is the longest street in the world - does it also contain the most nodes?\n",
    "pipeline = [{'$match': {'type': 'node',\n",
    "                        'address.street': {'$exists': 1} } },\n",
    "            {'$group': {'_id': '$address.street',\n",
    "                        'count': {'$sum': 1} } },\n",
    "            {'$sort': {'count': -1}},\n",
    "            {'$limit': 3}]\n",
    "pprint(aggregate(db, pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Ideas for Improvement\n",
    "\n",
    "The central tenet of this project, that human input leads to errors in data input which impedes data analysis, has made itself painfully clear in this project. Things like misplaced apostrophes lurk throughout, and typos like 'Commision' instead of 'Commission' lead to, in the output immediately above, 22 bus stops being miscategorized. Perhaps the simplest improvement that can be made is a parsing every key:value pair for typos via a dictionary/autosuggest library to probabilistically correct typos - such as, e.g., the NLTK library.\n",
    "\n",
    "The opening_hours field, investigated in detail in this project, was a very interesting and complex source of issues. Users submitted information in just about every format imaginable, and as the proper schema proposed by OSM has many rules and many special cases, parsing the values programatically is a challenge. There appears to be an open source Python script on Git, found at https://github.com/opening-hours/opening_hours.js/blob/master/regex_search.py, which attempts more sophisticated parsing that I've developed for this project, and it may be worthwhile to push my work here as a contribution toward that project.\n",
    "\n",
    "Perhaps the greatest challenge in working with this dataset is the profusion of missing data. As an example, for nodes containing addresses, the most common 'city' value is actually a missing value! In theory, this value should be derivable from the latitude and longitude of a given node, so long as there is a 'gold standard' value for municipal boundaries in these terms - and, as boundaries are seldom linear, this could take substantial manual labour to encode. Once defined, though, any node's address.city value can be easily determined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:\n",
    "1. https://docs.mongodb.com\n",
    "2. https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "3. https://regex101.com/\n",
    "4. http://stackoverflow.com/questions/7697710/python-running-out-of-memory-parsing-xml-using-celementtree-iterparse\n",
    "5. https://www.quora.com/What-is-the-best-way-to-parse-and-standardize-phone-numbers-in-Python/answer/Stuart-Woodward?srid=1FEf\n",
    "6. http://stackoverflow.com/questions/5906984/combine-days-where-opening-hours-are-similar"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
